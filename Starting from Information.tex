\documentclass{article}
\usepackage{graphics}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{setspace}
\author{Ruichen Wang \\
wangrc@2345.com}
\title{Starting from Information}
\begin{document}
\maketitle
\begin{abstract}
Starting with the introducing the origins of information, extend to entropy and its families. and some introduction and explanation of entropy related algorithms, like log-likelihood, softmax classification. Then we will talk about one basic algorithm solving intractable problems, which is called variational bayesian inference, and it's  closely related algorithms like latent dirichlet allocation(LDA), variational auto encoder(VAE), generative adversarial network(GAN),etc.
\end{abstract}

\tableofcontents
\section{What is Information?} 
\subsection{Defination of Information}
How to measure the information of certain event in a mathematics?

Given x is certain event, P(x) is probability which event x happens. Intuitively, the information should have inverse proportion to the probability, higher probability should have lower information, which is 
$$I(x)=\frac{1}{P(x)}$$
As $p(x) \in [0,1] $, we also want the information more stable, and remove the division for calculation convience, so we can re-define it as:
$$I(x)=log\frac{1}{P(x)}=-logP(x)$$
\subsection{Property of Information}
As a result, the $-logP(x)$ has every properties we want:
\begin{itemize}
\item Lower probability, higher information
\item Higher probability, lower information
\item Multi-event happens, the probability is multiplied. the information is summed
\end{itemize}
$$P(x_{1},x_{2})=P(x_{1})*P(x_{2})$$
$$logP(x_{1},x_{2})=logP(x_{1})+logP(x_{2})$$
$P(x) \in [0,1]$, and larger P(x) has smaller information.
\section{Entorpy (Expectation of Information)}
\subsection{Shannon's Information Theory}
Claude Elwood Shannon(1916-2001).\\
\noindent
1937 MIT Master degree.\\
1940 MIT Ph.D degree from MIT. \\
1948 Published a landmark paper 'A mathematical Theory of Communication'.\\
Entropy is defined as the expectation of information certain event carries:
$$H(x)=E[I(x)]=\sum_{i=1}^{n}P(x_{i})I(x_{i})=-\sum_{i=1}^{n}P(x_{i})logP(x_{i})$$
\subsection{Property of Entropy}
The property of the entropy is quite simple
\begin{itemize}
\item Higher probability, the less information, the lower entropy
\item Non-negative, every event has some information
\item Cumulative, multile events happens, the information is the sum of them.
\end{itemize}
\section{Families of Entropy}
\subsection{Cross-Entropy}
It is often the case that we don't know P(x) yet, so we make an 'artificial' probability distribution Q(x). How can we measure the cost as we using Q(x) to approximate P(x)? We define corss entropy as:
$$H(P,Q)=-\sum_{x}P(x)logQ(x)$$

\paragraph{In practice} Given a test set N observed, which comes from a Monte Carlo sampling of the true distribution P(x). Cross entropy is calculated using :
$$H(T,Q)=-\frac{1}{N}\sum_{i=1}^{N}logQ(x_{i})$$
\subsubsection{Relation to Log-likelihood}
for the maximum likelihood estimation (MAE), we want:
$$ \mathop{\arg\max} \prod_{i}q_{i}^{N_{p_{i}}}$$ 
So log-likelihood,divided by N is :
$$\frac{1}{N}log\prod_{i}q_{i}^{N_{p_{i}}}=\sum_{i}p_{i}logq_{i}=-H(p,q)$$
So maximum the likelihood is the same as minimizing the cross entropy.
\subsubsection{Cross-entropy Loss in Classification}
In machine learning, cross-entropy loss is widely used, it often defines as : 
$$L=-ylog(y^{'})=H(y,y^{'})$$

It describes the distance between the prediction and truth.
\subsubsection{Relationship with Softmax}
As the softmax probability and cross-entropy loss is so so common, and they often work together. But why? Because the simplicity of the derivative. \\
Softmax function:
$$p_{i}=\frac{e^{a_{i}}}{\sum_{k=1}^{N}e^{a_{k}}}$$
Derivative of softmax $\frac{\partial p_{i}}{\partial \alpha_{j}}$: \\
\begin{equation}
\nonumber
\frac{\partial p_{i}}{\partial \alpha_{j}}=\left\{
\begin{aligned}
p_{i}(1-p_{j}) & & i=j \\
-p_{j}*p_{i}  && i\neq j \\
\end{aligned}
\right.
\end{equation}
The cross entropy loss:
$$L=-\sum_{i}y_{i}logp_{i}$$
Derivative of cross entropy loss:
$$\frac{\partial L}{\partial o_{i}}=-\sum y_{k}\frac{1}{p_{k}}*\frac{\partial p_{k}}{\partial o_{i}}$$
From the dervative of softmax we derived earlier,
$$\frac{\partial L}{\partial o_{i}}=-y_{i}(1-p_{i})-\sum_{k \neq i}y_{k}\frac{1}{p_{k}}(-p_{k}*p_{i})$$
$$=p_{i}(y_{i}+\sum _{k \neq i}y_{k})-y_{i}=p_{i}-y_{i}$$
This is why we often use softmax and cross entropy together, The gradient is quite simple and elegant to calculate. 
\subsection{Kullback-Leibler Divergence}
KL divergence is also called relative entropy. It is a measure of how one probability distribution is different from a second.\\
For discrete probability distirbutions P and Q defined on the same probability space, the KL divergence from Q to P (Q with respect to P) is defined as :

\begin{align*}
D_{KL}(P \parallel Q)=H(P,Q)-H(P) \\
=-\sum_{i}P(i)log(\frac{Q(i)}{P(i)})
\end{align*}
Which means the more entropy using Q generates with respect to original distirbution P.

\subsubsection{Interpretations}
In machine learning, $D_{KL}(P \parallel Q)$ is often called the information gain achieved if Q is used instead of P.

Expressed in the language of Bayesian inference, $D_{KL}(P \parallel Q)$ is a measure of the information gained when one revises one's beliefs from the prior probability distribution Q to the posterior probability distribution P.

In applications, P typically represents the true distribution of data. Q represents the model. Minimize $D_{KL}(P \parallel Q)$ can be a good solution to find a Q that closest to P.  
\subsubsection{Property of KL}
\begin{itemize}
\item Non-negative \\
As a result known as Gibbs's inequality, with $D_{KL}(P \parallel Q)$ zero if and only if P = Q.
\item Asymmetric
$$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$
we can define symmetrised divergence as:
$$\frac{D_{KL}(P \parallel Q) + D_{KL}(Q \parallel P)}{2}$$
\end{itemize}
\subsubsection{Applications}
It is widely used in generative models. You can find it in NLP, computer vision, robotics, biology. We will use this in next section. Here I just put the bayesian equation here. :)
$$posterior=\frac{likelihood * prior}{evidence}$$
$$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$$

\section{Variational Bayesian Inference}
\subsection{Variational Inference}
The idea behind variational inference
\cite{doi:10.1080/01621459.2017.1285773} is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by \textbf{Kullback-Leibler divergence} .

Variational inference is widely used to approximate posterior densities for Bayesian models,an alternative strategy to Markov chain Monte Carlo (MCMC) sampling. MCMC algorithms try to sample a Markov chain, while the variational algorithms solve an optimization problem. variational inference tends to be faster and easier to scale to large data and high dimension.

Variational inference has a close relationship with EM algorithm. You can view VAE, GAN as a certain form of variational inference. Variational inference doesn't have the global optimal point, which make VAE/GAN are very hard to train or converge. That's why you can find a lot papers introducing how to train VAE/GAN more stable.
\paragraph{*Notes} Compare MCMC with Metropolis-Hasting (MH). MH larger the acceptance ratio $\alpha$. When extend to high dimensions, It is called Gibbs sampling. Actually, they are based on the same idea - \textbf{bayesian stationary distribution}. 

\paragraph{Question description} Suppose we have observations x, and hidden variables z, and some fixed parameters $\alpha$.What we want is the posterior distribution.
$$p(z|x,\alpha)=\frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)dz}$$
In many cases, the $\int_z p(z,x|\alpha)dz$ is intractable. we don't know how to compute it especially in high dimensions.
\paragraph{Solution} The main idea behind variational methods is to pick a family of distributions over the latent variables with its own \textbf{variational parameters}.
$$q(z_{1:m}|v)$$
Then find $v$ to make $q$ close to the posterior.
\subsection{KL Divergence Measure}
As mentioned above, we can use KL for this variational inference:
$$D_{KL}(q(z) \parallel p(z|x))=E_{q(z)} \left[ log \frac{q(z)}{p(z|x)} \right]$$
Intuitively, According to this formula, there are three cases:
\begin{itemize}
\item If q is low, then we don't care (Because of the expectation)
\item If q is high and p is high, good :)
\item If q is high and p is low, bad :(
\end{itemize}
\subsection{Evidence Lower Bound}
Actually we can not minimize KL divergence. But we can minimize another function which is equal to this. This is evidence lower bound (ELBO).
\subsubsection{Jensen's Inequality}
Jensen's inequality are widely used in EM algorithm. In convex function, we have :
$$f(tx_{1}+(1-t)x_{2})\leq tf(x_{1})+(1-t)f(x_{2}) $$ 
In the context of probability theory, if X is a random variable, and $\varphi$ is a convex function, then:
$$\varphi(E[x]) \leq E[\varphi(x)]$$
Back to the problem, we have observations $x^{1},x^{2},...,x^{n}$, we want $p(x^{i})$ get the max probability. Using MLE on it, which is the sum of the log-likelihood,
$$logp_{\theta}(x^{1},x^{2},...,x^{n})=\sum_{i=1}^{N}logp_{\theta} (x^{i})$$
and

\begin{align*}
logp(x) & =log \int_{z}p(x,z) \\
 &= log \int_{z}p(x,z)\frac{q(z)}{q(z)} \\
 &= log \left( E_{q} \left[ \frac{p(x,z)}{q(z)} \right] \right) \\
 &\geq E_{q} \left[ log \frac{p(x,z)}{q(z)} \right] \\
 &\geq E_{q}[logp(x,z)]-E_{q}[logq(z)]
\end{align*}
Note the second term is the entropy. Now we have the evidence lower bound EBLO:
$$log p(x) \geq E_{q}[logp(x,z)]+H(z)$$
But what does this have to do with the KL divergence? 
\subsubsection{KL Transformation}
As mentioned, we want $q(z)$ and $q(z|x)$ are close to each other:
\begin{align*}
KL(q(z)||p(z|x)) &= E_{q}\left[ log \frac{q(z)}{p(z|x)} \right] \\
&=E_{q}[logq(z)]-E_{q}[logp(z|x)]
\end{align*}
As we know,
$$p(z|x)=\frac{p(z,x)}{p(x)}$$
so 
\begin{align*}
KL(q(z)||p(z|x)) &= E_{q}[logq(z)]-E_{q}[logp(z,x)]+E_{q}[logp(x)] \\
&= -(E_{q}[logp(z,x)]-E_{q}[logq(z)])+logp(x)
\end{align*}
The first term is ELBO we just met.\\
The formula can also be written as :
$$logp(x)=KL(q(z)||p(z|x))+(E_{q}[logp(z,x)]-E_{q}[logq(z)])$$

As I mentioned before, For two different distributions, KL divergence is always non-negative. and $p(x)$ is the observation evidence, which is fixed. So minimizing the KL divergence is the same as maximizing the ELBO. This is also called as the variational lower bound.
\subsubsection{Relationship with EM}
EM algorithm is also known as a famous method to find the distirbutions of latent varibales. Unlike variational inference we are going to talk about, EM algorithm use the fact that ELBO is equal to the $p(x)$ when $q(z)=p(z|x)$. EM \textbf{alternates} between computing $p(z|x)$ (E step), and optimizing it with respect to the model parameters(M step). The biggest difference is EM assume $p(z|x)$ is computable and fix the parameter, use it, while variational inference use bayesian setting and apply to the models we can not compute. 

* EM is out the scope of this article, I don't want to go into too detail about it. Actually the formula below explains pretty clear.\\
E-step:
$$q(z):=p(z|x;\theta)$$
M-step:
$$\theta:=\mathop{\arg\max}_{\theta} -KL(q(z)||p(z|x;\theta))$$


\subsection{Mean Field Theory}
Mean field theory is also called \textbf{self-consistent field theory}. It studies the behavior of large and complex stochastic models by studying a simpler model.Such models consider a large number of small individual components that interact with each other.

The effect of all the other individuals on any given individual is approximated by a single averaged effect, thus reducing a many-body problem to a one-body problem.
\subsubsection{Mean Field Approximation}
We assume each variable is independent.Using this theory, we can write:
$$q(z_{1:m})= \prod _{i=1}^{m}q(z_{i})$$
$$E_{q}[logq(z_{1:m})]=\sum_{j=1}^{m}E_{q_{j}}[logq(z_{j})]$$
Also we have the chain rule 
$$p(z_{1:m},x_{1:n})=p(x_{1:n}) \prod_{j=1}^{m}p(z_{j}|z_{1:(j-1)},x_{1:n})$$
\subsubsection{Mean Field Method}
Note that the order of j is irrelevant.Based on this theory, we can rewrite the lower bound as:
\begin{align*}
\mathcal{L} &= logp(x_{1:n})+\sum_{j=1}^{m} E_{q}[logp(z_{j}|z_{1:(j-1)},x_{1:n})]-E_{q_{j}}[logq(z_{j})] 
\end{align*}
Consider the variable $z_{j}$ comes last: 
$$\mathcal{L} =logp(x_{1:n})+E_{q}[logp(z_{j}|z_{-j},x)]-E_{q_{j}}[logq(z_{j})]$$
And we can remove the first term because it's irrevalte to $q(z_{j})$,the $\mathcal{L}$ can be written as

\begin{align*}
\mathop{\arg\min}_{q_{j}} \mathcal{L} &=E_{q}[logp(z_{j}|z_{-j},x)]-E_{q_{j}}[logq(z_{j})] \\
&= \int q(z_{j})E_{-j}[logp(z_{j}|z_{-j},x)]dz_{j}-\int q(z_{j})logq(z_{j})dz_{j}
\end{align*}

\subsection{Coordinate Ascent Variational Inference}
Let's treat $q(z_{j})$ as $f(x)$.
For simplicity, I convert the formula above into this:
$$\frac{d \mathcal{L}}{dq(z_{j})}=\frac{d[\int Kf(x)dx- \int f(x)logf(x)dx]}{d[f(x)]}=0$$
this is equal to:
$$\frac{d[\int Kf(x)dx- \int f(x)logf(x)dx]}{dx} \times \frac{•}{•}{dx}{d[f(x)]}=0$$
$$ [ K f(x)-f(x)logf(x)] \times \frac{1}{f'(x)}=0$$
which is :
$$Kf'(x)-[f'(x)logf(x)+f(x) \frac{1}{f(x)}f'(x)]  =0$$
and:
$$K-logf(x)-1=0$$
which means the argmax of ELBO can be find at: 
$$E_{-j}[logp(z_{j}|z_{-j},x)]-logq(z_{j})-1=0$$
$$log \frac{ e^{E_{-j}[logp(z_{j}|z_{-j},x)]}}{q(z_{j})}= log_{e}e $$
Or you can simply view it as $y-x-1=0$.
This lead to the conclusion:
$$q^{*}(z_{j}) \propto exp \left\{ E_{-j}[logp(z_{j}|z_{-j},x)] \right\}$$
since $p(z_{j}|z_{-j},x)=\frac{p(z_{j},z_{-j},x)}{p(z_{-j},x)} $, and $p(z_{-j})$ does not depend on $z_{j}$ we can equivalenty write:
$$q^{*}(z_{j}) \propto exp \left\{ E_{-j}[logp(z_{j},z_{-j},x)] \right\}$$

\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\renewcommand{\algorithmicensure}{ \textbf{Output:}}

\begin{algorithm}
\caption{Coordinate Ascent Variational Inference}
\begin{algorithmic}
\REQUIRE A model $p(x,z)$, a dataset x
\ENSURE A variational density $q(z)=\prod_{j=1}^{m}q_{j}(z_{j})$
\STATE {\textbf{init:}  variational factors $q_{j}(z_{j})$} 
\WHILE {\emph{ELBO has not converged}}
\FORALL {$j \in \left\{1,...,m  \right\}$ }
\STATE set $q_{j}(z_{j}) \propto exp \left\{ E_{-j}[logp(z_{j},z_{-j},x)] \right\}$ 
\ENDFOR
\STATE compute $ELBO=E_{q}[logp(z,x)]-E_{q}[logq(z)]$ 
\ENDWHILE 
\end{algorithmic}
\end{algorithm}

Note that there is generally no guarantee of convexity of ELBO, this coordinate ascent procedure converges to a local maximum.

We can find this method is closely related to Gibbs sampling. Actually Gibbs sampling is a very classical approximate inference method. In variational inference,
we take the expected log and set each variable’s variational factor iteratively.
\section{Latent Dirichlet Allocation}
LDA\cite{geigle2016inference} is a conditionally conjugate topic model comparing to PLSA. It treats documents as containing multiple topics, where a topic is a distribution over words in vocabulary. 

Before we introduce the relationship with variational inference, let's go through some basic knowledge.
\subsection{Conjugate Distributions}
In Bayesian probability theory, if the posterior distirbution $p(\theta | x)$ is in same probability distirbution family as the prior distribution $p(\theta)$, the prior and posterior are then )]called \textbf{conjugate distributions}. And the prior is called \textbf{conjugate prior} for the likelihood function.

\subsubsection{Beta-Binomial Distribution}
Beta distribution is a conjugate prior for binomial distribution. For better understanding, Let's consider a simple problem here. 
\paragraph{Question 1} For $X_{1},...,X_{n} \sim Uniform(0,1)$, what is the distirbution of $Kth$ largest number? 
\paragraph{Answer 1} Let's assume the $Kth$ largest number locates in $[x,x+\Delta x]$. So there are K-1 numbers locate in $(0,x)$, and n-K numbers locate in $(x+\Delta x,1)$. Descirbe this in math: 
$$P(x \leq X_{k} \leq x+\Delta x)=n \binom {n-1} {k-1} x^{k-1}(1-x-\Delta x)^{n-k}\Delta x$$
Convert to PDF, set $\alpha=k, \beta=n-k+1$:
$$f(x)= \lim_{\Delta x \rightarrow 0} \frac{P(x \leq X_{k} \leq x+\Delta x)}{\Delta x} =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} $$
$$f(x)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}, B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$$
This can also be denoted by:
$$X \sim Beta(\alpha,\beta)$$

\paragraph{Question 2} For $X_{1},...,X_{n} \sim Uniform(0,1)$, what is the distirbution of $Kth$ largest number? \\
Given the knowledge: $Y_{1},...,Y_{m} \sim Uniform(0,1)$, there are $m_{1}$ $Y_{i}$ smaller than $X_{k}$, $m_{2}$ $Y_{i}$ larger than $X_{k}$.

\paragraph{Answer 2}
Describe the quesion in math, we have prior $X_{k} \sim Beta(\alpha,\beta)$,likelihood $m_{1} \sim B(m,X_{k})$, the posterior PDF now can be written as:
$$X \sim Beta(\alpha+m_{1},\beta+m_{2})$$
Actually you can treat X, Y as $X_{n},X'_{m} \sim Uniform(0,1)$. This question is the same as the previous one.

\subsubsection{Dirichlet-Multinomial Distribution}
Dirichlet-multinomial distribution is also called as ploya distribution. It is just a multivariate extension of beta-binomial distribution. Dirichlet is the high dimensional Beta distribution, like binomial to multivariate.
$$f(x)=\frac{1}{B(\bm{\alpha} )}\prod_{i=1}^{K}x_{i}^{\alpha_{i}-1}$$
where 
$$B(\bm{\alpha})=\frac{\prod_{i=1}^{K} \Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{K}\alpha_{i})}$$
And we also have the same property:
$$Prior_{Dir} + Likelihood_{Multi} = Posterior_{Dir}$$
$$Dir(p|\bm{\alpha})+Multi(\bm{m})=Dir(p|\bm{\alpha+m})$$
\subsection{Steps of Smoothed LDA}
Original LDA Suppose there exists K topics among all documents. Each document j in M is a mixture of these topics, controled by $\theta_{j}$. And then we can  generate words for $d_{j}$ by first sampling a topic $z_{j,t}$ from $\theta_{j}$. And then sampling a word from corresponding topic $\phi_{z_{j,t}}$.

The original LDA has a obvious weakness that it does not have prior for each $\phi_{k}$.  Smoothed LDA choose dirichlet parameterized by $\beta$ as the prior. and is commonly used now.

Which is equal to the following steps: 

\begin{enumerate}
\item 
{ For each topic in i = 1,...,$K$: 
\begin{enumerate}
\item draw a distribution over words $\phi_{i} \sim Dir_{V}(\beta)$
\end{enumerate}
}
\item
{ For each document in j=1,...,$M$:
\begin{enumerate}
\item draw a vector of topic proportions $\theta_{j} \sim Dir_{K}(\bm{\alpha})$
\item For each word in t = 1,..., $N_{j}$:
{
\begin{enumerate}
\item  draw a topic assignment $\bm{z}_{j,t}^{k} \sim Mult(\theta_{j})$
\item draw a word $w_{j,t}^{v} \sim Mult(\phi_{\bm{z}_{j,t}^{k}})$
\end{enumerate}
}
\end{enumerate}
}
\end{enumerate}
Here $\beta$ is a fixed parameter of dirichlet prior on topics $\phi$ with respect to $V$. and $\bm{\alpha}$ are fixed parameters of dirichlet prior of topics on each document.

Which can also be denoted as:
$$p(\theta,z,\phi,w|\alpha,\beta)=\prod_{i=1}^{K} p(\phi_{k}|\beta)\prod_{j=1}^{M}\left( p(\theta_{j}|\alpha)\prod_{t=1}^{N_{j}} p(z_{j,t}|\theta_{j})p(w_{j,t}|\phi_{z_{j,t}})\right)$$

\subsection{Variational Inference for LDA}
Remember Section 4.1, we mentioned that a typical variational inference problem is: 
$$p(z|x,\alpha)=\frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)dz}$$
where $\alpha$ is fixed parameter and z is hidden variable. if we find $\int_z p(z,x|\alpha)dz$ intractable, we just create a new distribution $q(z|v)$ to approximate the posterior.\\
\noindent
Here for LDA, we have:
$$p(\theta,\phi,z|w,\alpha,\beta) = \frac{p(\theta,\phi,z,w|\alpha,\beta)}{\int_{\theta} \int_{\phi} \sum_{z} p(\theta,\phi,z,w|\alpha,\beta)d_{\phi}d_{\theta} }$$
where $\alpha$,$\beta$ are some fixed parameters, and $\theta,\phi,z$ are hidden parameters. 

The topic assignments $z$ and their prior distirbution $\theta$ are in conjugate relationship, which is good. But the introduction of $\phi$ is in coupling with $z$ which makes the posterior intractable.

So we can build a tractable distribution $q(\phi,\theta,z|\lambda ,\gamma, \pi)$, with variational variable $\lambda ,\gamma, \pi$
\begin{align*}
q(\phi,\theta,z|\lambda ,\gamma, \pi) &= \prod_{i=1}^{K}q(\phi_{i}|\lambda_{i}) \prod_{j=1}^{M}\left(q(\theta_{j}|\gamma_{j})\prod_{t=1}^{N_{j}}q(z_{j,t}|\pi_{j,t})\right)
\end{align*}
And we can measure the KL divergence between $p(\cdot)$ and $q(\cdot)$:
\begin{align*}
\mathcal{L} &= \mathop{\arg\min}_{\lambda ,\gamma, \pi} KL(q(\phi,\theta,z|\lambda ,\gamma, \pi)||p(\theta,\phi,z|w,\alpha,\beta))
\end{align*}

\begin{align*}
\mathop{\arg\max}_{\lambda ,\gamma, \pi} ELBO =\sum_{i=1}^{K}E_{q}[log p(\phi_{i}|\beta)]+\sum_{j=1}^{M}E_{q}[log p(\theta_{j}|\alpha)] \\
+\sum_{j=1}^{M}\sum_{t=1}^{N_{j}}E_{q}[logp(z_{j,t}|\theta_{j})] 
+\sum_{j=1}^{M}\sum_{t=1}^{N_{j}}E_{q}[logp(w_{j,t}|\phi_{z_{j,t}})]
+H(q)
\end{align*}
$$H(q)=-E_{q}[logq(\phi|\lambda)]-E_{q}[logq(\theta|\gamma)]-E_{q}[logq(z|\pi)]$$
Great! Just use Lagrange multiplies, problem solved!:) \\

Actually as we know LDA is a conditionally conjugate model. We can directly identify the family of each variable:
$$q(\phi|\lambda) \sim Dir_{v}(\lambda) \sim Dir_{v}\left(\beta+\sum_{j=1}^{M}\sum_{t=1}^{N}z_{j,t} w_{j,t} \right)$$
$$q(\theta|\gamma) \sim Dir_{k}(\gamma) \sim  Dir_{k}(\alpha+\sum_{t=1}^{N}z_{j,t}) $$
$$q(z|\pi) \propto exp(log \theta_{j,k}+log \phi_{k,w_{dn}})$$
This is just the same question as section 4.3.2. We can apply the same algorithm to make the variables converge.
\paragraph{E-step:} for each document j, each word t, compute $\pi, \gamma$ until converge
\paragraph{M-step:} using $q(\cdot)$ re-estimate $\lambda$

\bibliographystyle{plain}
\bibliography{ref}


\end{document}


























