\documentclass{article}
\usepackage{graphics}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\author{Ruichen Wang}
\title{TODO}

\begin{document}
\maketitle


\centerline{\textbf{Some Basics}}

C4.5, ID3, CART, K-means, SVM, Kernel function, SMO, EM, Naive bayes, Variational inference, PageRank, Adaboost, KNN, Data cleaning, Linear regression, Logistic regression, LDA, PCA, Random Forest, Bagging \& stacking, Softmax, GBDT, Xgboost, LightGBM, PCA, Max-entorpy, Gradient diminishing, FM, LFM, MF, SVD, SVD++, Simhash, Max likelihood, ALS, L1 norm, L2 norm, Discriminate \& Generative model, Entorpy, Cross-entropy, KL divergence, SGD, BGD, MBGD, Adam, Newton's method, Unbiased estimator, F1 score, Recall, Prercision, AUC, ROC, Cross validation, Bias-Variance tradeoff, Loss function, Overfitting \& underfitting, DBSCAN, TF-IDF, Text rank, Word2vec, Glove, Fast-text, Collaborative Filtering, User-cf, Item-cf, Dropout, Batch norm, Maxpooling, Avg pooling, Global avg pooling, Respective field, LR normalization, Online learning, FTRL, FOBOS, RDA, Tree row \& column sampling,  ANN, Tree normalization, RNN, LSTM, GRU, CRF, RNN \& CNN, KDtree, Resnet, Seq2seq, Deep wise CNN, 1X1 kernel, SVM multi-classification, Attention, Conv complexity, Pearson correlation, Gibbs sampling, t-SNE.

\centerline{\rule{\textwidth}{1pt}}

\section{ID3, C4.5, CART difference?}
What is Decision Tree (DT)? \\
DTs are non parametric superivised learning method, can be used for classification and regression. DTs need little data preparation. No need NOrmalization. White box model. Easily overfitting, sensitive to variation, because may build complete different trees, can be mitigated by using ensemble. Based on greedy opt to solve NP-complete problem, may reach local optimal. Need balance dataset. \\
\begin{figure}[H]
\centering
\includegraphics[width=4in,height=3in]{dt}
\caption{A decision tree using gini.}
\end{figure} 

Pratical Tips:
\begin{itemize}
\item DT tend overfit if using a large number of features, Using a ratio to sample the features.
\item Consider perform feature dimensionality reduction (PCA, Feature selection).
\item Control max depth, control min samples per leaf to prevent overfitting
\end{itemize}
ID3 (info gain) \\
\textbf{Entropy $H(S)$} is a measure of uncertainty of data. 
$$H(S)=\sum_{x \in S}-p(x)logp(x)$$
\textbf{Information Gain $IG(S)$} is a measure of differenece in entropy before and after $S$ splited on attribute A.
$$IG(S,A)=H(S)-\sum_{t \in A}p(t)H(t)=H(S)-H(S|A)$$
C4.5 (gain ratio)
C4.5 is the successor to ID3. Can handle continuous attribute.
$$GainRatio(S,A)=\frac{IG(S,A)}{IV(A)}$$
CART(Gini impurity)
CART using binary split, Split based on one variable. Pruning mainly has two types : pre and post. CART use CCP (cost complexity pruning), which is defined by $\frac{error rate}{tree complexity}$
$$Gini(A,S)=1-\sum_{t \in A} p_{t}^{2}$$


\section{Why L1 regulation generates sparsity? L2 regulation cause blur?}

Firstly, why do we want the result matrix to be sparse?

Consider 1 million dimension, calculate the inner product between $w$ and $x$ need a lot of computation. If the $w$ can be sparse, the inner product will only be performed on the non-zero columns.

Or consider another situation, in some scenario, there are free data and many features, which is often called as \textbf{`small n, large p problem'}. If $n \ll p $, then our model will be very complex, our $w$ will be a singular matrix ($|w|=0$). In other words, \textbf{overfitting}.

One way to control overfitting is adding a regularization term to the loss function. Rigde  ($l_{2} norm$) and LASSO ($l_{1} norm$) regression are two very common regression ways.
$$J(w)=Loss(x)+\lambda ||w||_{2}^{2}$$
$$J(w)=Loss(x)+\lambda ||w||_{1}$$
Assume we use loss using MSE, the target function can also be denoted as :
$$ \mathop{\min}_{w} \frac{1}{n}||y-Xw||^{2} \quad  s.t. \lambda ||w||_{2}^{2} \leq C $$
$$ \mathop{\min}_{w} \frac{1}{n}||y-Xw||^{2} \quad  s.t. \lambda ||w||_{1} \leq C $$

Back to the problem, intuitivly, the target loss will alway intersect at the coordinate axis when using l1 norm. Imaging high dimension situation, the angles will certainly more likely to be intersected, while the ball will not.

\begin{figure}[H]
\centering
\includegraphics[width=3.5in,height=1.7in]{l1l2}
\caption{L1 and L2 norm.}
\end{figure}

For more math proof, see \textit{http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization}

\paragraph{Why L2 regulation cause blur?}

In generative models, eg.VAE, L2 norm / L2 loss / MSE tend to yield blurry images. We try to explain this in probabilistic settings. In Gaussian distribution, it defines as :
$$p(x|\mu,\sigma^{2})=\frac{1}{Z}exp\left( -\frac{||\mu-x||^{2}}{2\sigma^{2}}\right)$$
$$logp(x|\mu,\sigma^{2}) \propto exp\left(-\frac{1}{2}||x_{\mu}-x||^{2}  \right)$$
As we can see, minimizing MSE, is same as maximizing the log likelihood of gaussian, we make the assumption that our $x$ comes from a gaussian.


\begin{figure}[H]
\centering
\includegraphics[width=3in,height=1.7in]{l2}
\caption{Gaussian and multinomial.}
\end{figure}

In reality, there often many hidden variables (multinomial) controls the $x$. A simple example, we have white and black dogs as dataset $x$. maximizing the likelihood will blur the two and generate gray dogs.

\section{One-hot encoding for gbdt?}

It is often the case that we have continuous and categorical features. One-hot encoding may produce very sparse variables. Tree based algorithm tries to increase information gain on the data it`s splitting at any given level. If the data is very sparse, the one-hot encoded feature may be ignored as they are far too sparse. 

\textit{Then how to using categorical features  in trees?} 

In xgboost \cite{DBLP:conf/kdd/ChenG16}, it treat every input as numerical, It may be helpful is the categories as small. Otherwise, it may downgrade the performance. 

In lightGBM, it said that a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy. It`s automaticly provide a optimal solution $O(k*logk)$ to find the best optimal partition. This often performs better than one-hot encoding. 

For more info, see lightgbm api.

\section{xgboost, lightGBM, gbdt, GBM?}

Gradient tree boosting is also known as gradient boosting machine (GBM) or gradient boosted regression tree (GBRT). 

\textbf{xgboost} using taylor expansion to approximate the objective function. Regularization is defined as number of leafs plus l2 norm of leaf values.
 
\begin{figure}[H]
\centering
\includegraphics[width=4in,height=1.5in]{taylor}
\caption{}
\end{figure}

for detail see pdf in ml papers basis.

\textbf{lightgbm} choose the leaf with max delta loss to grow(leaf wise, best first), while most tree grow by level wise. Using max depth to control overfitting.

\section{GD, BGD, SGD, momentum, adagrad, adam}
GD refers to gradient descent, Using to negative gradient to optimize the ojbective function.
Batch gradient descent (BGD) computes the gradient on the whole dataset, which if often slow and intractable. Can not be applied to online model which data is on the fly. 
$$\theta=\theta - \eta \cdot \nabla_{\theta} J(\theta)$$
BGD guaranteed to converge to the global minimum for convex problem.

SGD perform an update for each training example. Usually slowly decrease the learning rate.
$$\theta=\theta - \eta \cdot \nabla_{\theta} J(\theta; x^{i};y^{i})$$

Mini-batch gradient descent perform an update for every mini batch of $n$ train examples. 

Momentum adagrad,adam, rmsprop etc are all opt strategy.

\section{EM Algorithm and VI}
In the topic of clustering, it can be classified as hard and soft clustering. EM is a soft clustering method which is a generative model which trys calculate parameters of probability distribution. 
EM algorithm is used for maximum likelihood estimates of parameters. In bayesian statistics, it is often used to obtain the mode of the posterior marginal distributions of parameters.

\textbf{E-step} computes the expected value of $l(\theta;X,Z)$ given the observed data $X$, and current estimate $\theta_{old}$.

\begin{align*}
Q(Z;\theta_{old}) &= P(Z| X,\theta_{old})
\end{align*}

\textbf{M-step} maximizing over $\theta$ the expectation computed.
\begin{align*}
\theta_{new} :&= \mathop{\arg\max}_{\theta} -KL(Q(Z) || P(Z|X,\theta)) \\
&= \mathop{\arg\max}_{\theta} \sum_{i}\sum_{z^{i}} Q_{i}(z^{i})log \frac{p(x^{i},z^{i};\theta)}{Q_{i}(z^{i})}
\end{align*}

In variational inference, we know that 
$$logp(x)=KL(q(z)||p(z|x))+(E_{q}[logp(z,x)]-E_{q}[logq(z)])$$
The difference of EM and VB is the kind of results they provide: \textbf{EM is just a point, VB is a distribution}. However, they also have similarities. EM and VB can both be interpreted as minimizing some sort of distance between the true value and our estimate, which is the Kullback-Leibler divergence. \\
For example:

When the Gaussian model involves latent variables and parameters only, Expectation Maximization is enough to solve the model. 

If, on top of the latent variables, the parameters becomes random with prior distributions, the Variational Inference (or Variational Bayes) method is used.

\bibliographystyle{plain}
\bibliography{ref}
\end{document}

























